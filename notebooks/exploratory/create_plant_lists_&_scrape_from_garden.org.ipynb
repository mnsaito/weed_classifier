{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Datasets\n",
    "## Plant Lists\n",
    "- I scraped a list of the best performing perennials in the midwest from midwestgardentips.com\n",
    "- I scraped a list of common weeds in IL from preen.com, which provides a list of common weeds state-by-state\n",
    "\n",
    "## Photo Collection\n",
    "- I scraped the photos of weeds available on preen.com\n",
    "- I attempted to scrape photos of both the perennials and weeds from garden.org, but due to excessive scraping, garden.org blocked me from scraping all of the photos\n",
    "    - As a back-up, I scraped the photos of the perennials from the Missouri Botanical Gardens, and the weeds from UMass (the code for which is provided in separate notebooks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# webscrape\n",
    "import requests\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to store data\n",
    "perennial_path = os.path.join(os.pardir, os.pardir, 'data', 'perennials')\n",
    "weed_path = os.path.join(os.pardir, os.pardir, 'data', 'weeds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## midwestgardentips.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape names of best performing perennials from Midwest Gardening site\n",
    "perennial_url = 'https://www.midwestgardentips.com/best-performing-perennials-1'\n",
    "response = get(perennial_url)\n",
    "html = response.text\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "# Names of plants appear to be bolded (i.e., 'strong') and italicized (i.e., 'em')\n",
    "p_list = [a.text for a in (strong.find('em') for strong in soup.find_all('strong')) if a]\n",
    "\n",
    "perennials = []\n",
    "for i in range(len(p_list)):\n",
    "    text = p_list[i].split(':')[0]\n",
    "    perennials.append(text)\n",
    "\n",
    "# Remove mislabeled text from list of perennials\n",
    "perennials.remove('y.')\n",
    "perennials.remove('Full to part sun\\xa0 Hardy in zones ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preen.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of weeds and\n",
    "# Scrape weed photos from preen site\n",
    "\n",
    "weed_url = 'https://www.preen.com/weeds/il'\n",
    "response = get(weed_url)\n",
    "html = response.text\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "div = soup.find(id = 'WeedList')\n",
    "w_list = div.find_all('a')\n",
    "weeds = []\n",
    "for i in range(len(w_list)):\n",
    "    # Create list of weed names\n",
    "    text = w_list[i].find('img').attrs['alt']  \n",
    "    weeds.append(text)\n",
    "    \n",
    "    # Scrape photos from site\n",
    "    photo_url = 'https://www.preen.com' + w_list[i].attrs['href']\n",
    "    photo_response = get(photo_url)\n",
    "    photo_html = photo_response.text\n",
    "    photo_soup = BeautifulSoup(photo_html, 'lxml')\n",
    "    photo_div = photo_soup.find(id = 'imagePicker')\n",
    "    photo_list = photo_div.find_all('a')\n",
    "    for j in range(len(photo_list)):\n",
    "        photo_url = 'https:' + photo_list[j].attrs['href'].replace(' ', '%20')\n",
    "        \n",
    "        # To account for photos that were removed from the site\n",
    "        if get(photo_url).status_code != 404:\n",
    "            \n",
    "            # \"pr\" suffix to indicate photos were scraped from preen site\n",
    "            path = os.path.join(weed_path, text.lower().replace(' ', '_') + '_pr')\n",
    "            urllib.request.urlretrieve(photo_url, path + '_' + str(j) + '.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## garden.org\n",
    "I created a function (enter_url) that provides the name of each perennial/weed one at a time.  The first prompt allows the user to skip the perennial/weed if no photos are provided for the plant on garden.org.  If photos exist, the user can enter the url for the plant.  The function scrapes all photos where the plant name is identified in the header or as a common name for the plant.  All plants are stored with the plant name as part of the name of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get soup from garden.org site\n",
    "def get_soup(url):\n",
    "    # Need to \"fake a browser visit\" by providing a user-agent header for garden.org\n",
    "    response = requests.get(url, headers = {'User-Agent' : 'test'}, proxies = {'http' : proxy, 'https' : proxy})\n",
    "    html = response.text\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    return soup\n",
    "\n",
    "# garden.org provides a \"results page\" when searching for a plant\n",
    "# Each \"result\" includes a link that provides photos for the plant\n",
    "# Function goes to the URL for each result on page, and calls the \"add_plant\" function\n",
    "def get_results(result_soup, plant_name, count, weed):\n",
    "    find_plants_results = result_soup.find('table')\n",
    "    plants_results = find_plants_results.find_all('tr')\n",
    "    # Create list of URLs for each result\n",
    "    for k in range(len(plants_results)): # For each result\n",
    "        plant_url = 'https://garden.org' + plants_results[k].find('a').attrs['href']\n",
    "        # Count keeps track of the number of photos for each plant\n",
    "        count = add_plant(plant_url, plant_name, count, weed)\n",
    "    sleep(1)\n",
    "    return (count)\n",
    "\n",
    "# Function adds all photos from each \"result\"\n",
    "# \"Results\" include plants that contain the search term\n",
    "# Only plants that match the name of the search term \n",
    "# as a \"common name\" for the plant or in the header of the page are included\n",
    "def add_plant(plant_url, plant_name, count, weed):\n",
    "    soup = get_soup(plant_url)\n",
    "    if weed:\n",
    "        path = os.path.join(weed_path, plant_name)\n",
    "    else:\n",
    "        path = os.path.join(perennial_path, plant_name)\n",
    "    \n",
    "    # Create list of common names\n",
    "    tables = soup.find_all('table')\n",
    "    common_names_table = None\n",
    "    for j in range(len(tables)):\n",
    "        if tables[j].find('caption'):\n",
    "            if 'common' in tables[j].find('caption').text.lower():\n",
    "                common_names_table = tables[j]\n",
    "    common_names_list = []\n",
    "    if common_names_table:\n",
    "        common_names = common_names_table.find_all('tr')\n",
    "        for k in range(len(common_names)):\n",
    "            common_names_list.append(common_names[k].find('td').findNextSibling().text.strip().lower())\n",
    "                \n",
    "    # Add names in header to list of common names\n",
    "    header_names = soup.find('h1', {'class' : 'page-header'}).text.lower()\n",
    "    header_names = header_names.replace('(', '→').replace(')', '').split('→')\n",
    "    common_names_list += header_names\n",
    "    \n",
    "    # If search term is in header or list of common names, add photos\n",
    "    if plant_name.replace('_', ' ') in common_names_list:\n",
    "        photo_gallery = soup.find_all('div', {'class' : 'plant_thumbbox'})\n",
    "        for i in range(len(photo_gallery)):\n",
    "            photo_url = 'https://garden.org' + photo_gallery[i].find('a').find('img').attrs['src']\n",
    "            if get(photo_url).status_code != 404:\n",
    "                urllib.request.urlretrieve(photo_url, path + '_' + str(count) + '.jpg')\n",
    "                count += 1\n",
    "    return (count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find URLs for results page and pull all plants for each result\n",
    "# \"data\" is the list of perennials or weeds\n",
    "# \"weed\" indicates whether the plant is a weed (weed=True) or not\n",
    "def enter_url(data, weed):\n",
    "    for l in range(len(data)):\n",
    "        print('Plant:  ', data[l])\n",
    "        add_photos = input('Add Photos? (Y/N):  ')\n",
    "        if (add_photos == 'Y') or (add_photos == 'y'):\n",
    "            plants_url = input('Enter garden.org url:  ')\n",
    "            plant_name = plants_url.split('=')[-1].replace('+', '_')\n",
    "            count = 0 # Track number of results to name plant\n",
    "            \n",
    "            # Go to URL for each result on page, and add plants from each\n",
    "            plant_soup = get_soup(plants_url)\n",
    "            count = get_results(plants_soup, plant_name, count, weed)\n",
    "            \n",
    "            # Check if there are additional results pages\n",
    "            # Will return actual page if one exists. Otherwise, will return nothing.\n",
    "            query = plant_soup.find('span', {'class' : 'PageActive'})\n",
    "            if query:\n",
    "                next_page = query.findNextSibling()\n",
    "                while next_page:\n",
    "                    next_url = 'https://garden.org' + next_page.attrs['href'] # Go to next page of results\n",
    "                    plant_soup = get_soup(next_url)\n",
    "                    count = get_results(plant_soup, plant_name, count, weed)\n",
    "                    query = plant_soup.find('span', {'class' : 'PageActive'})\n",
    "                    if query:\n",
    "                        next_page = query.findNextSibling()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enter_url(perennials, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "enter_url(weeds, True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
